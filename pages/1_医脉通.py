import streamlit as st
import os

from utils.tools import load_info
from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from utils.tools import read_prompt_file, get_context_from_db
from operator import itemgetter
from langchain_core.runnables import RunnableLambda
from time import time

# LangChain é…ç½®
os.environ["LANGCHAIN_TRACING"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "AIGC"
os.environ["LANGCHAIN_API_KEY"] = load_info("keys")["LANGCHAIN_API_KEY"]
os.environ["DASHSCOPE_API_KEY"] = load_info("keys")["DASHSCOPE_API_KEY"]

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(page_title="åŒ»è„‰é€š", page_icon="ğŸ¤–")

st.sidebar.info(
    "æ¬¢è¿ä½¿ç”¨åŒ»è„‰é€šæ™ºèƒ½è¯Šç–—ç³»ç»Ÿï¼\n\n"
)

st.sidebar.markdown("### åŠ©æ‰‹é…ç½®")
dept = st.sidebar.selectbox("é€‰æ‹©ç§‘å®¤", ("ç”·ç§‘", "å†…ç§‘", "å¦‡äº§ç§‘", "è‚¿ç˜¤ç§‘", "å„¿ç§‘", "å¤–ç§‘"))

# è¯»å–prompt
system_prompt = read_prompt_file(dept)

# æ¨¡å‹åˆ—è¡¨
models = load_info("models").keys()

# st.sidebar.markdown("### æ¨¡å‹é…ç½®")
selected_model = st.sidebar.selectbox("é€‰æ‹©å¤§æ¨¡å‹", models)
rag_flag = st.sidebar.checkbox("æ˜¯å¦ä½¿ç”¨æ£€ç´¢åº“")
temperature = st.sidebar.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
max_tokens = st.sidebar.slider("æœ€å¤§ç”ŸæˆTokenæ•°", min_value=10, max_value=2048, value=512, step=10)

st.sidebar.markdown("### é…ç½®è¯´æ˜")
st.sidebar.info(
    "æ¸©åº¦ï¼šæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šéšæœºã€‚\næœ€å¤§ç”ŸæˆTokenæ•°ï¼šæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ï¼Œå€¼è¶Šé«˜ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¶Šé•¿ã€‚"
)

# è‡ªå®šä¹‰CSSï¼Œç¾åŒ–ç•Œé¢
st.markdown("""
<style>
    .stApp {
        background_color: #f0f2f6;
    }
    .stChatInputContainer > div {
        background-color: white;
    }
    [data-testid="stChatMessageContent"] {
        background-color: #e6e6fa;
        border-radius: 0.5rem;
        padding: 0.75rem;
    }
    [data-testid="stChatMessageContent"][aria-label="assistant message"] {
        background-color: #d1e7dd; /* æµ…ç»¿è‰²èƒŒæ™¯ */
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 0.5rem;
    }
    .stButton>button:hover {
        background-color: #45a049;
    }
    h1 {
        color: #333;
    }
</style>
""", unsafe_allow_html=True)

st.title(f"ğŸ’¬ åŒ»è„‰é€š{dept}åŠ©æ‰‹")
st.caption(f"[{selected_model}] Powered by AIE-52 G5")

# åˆå§‹åŒ–èŠå¤©è®°å½• (ä½¿ç”¨ session_state)
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": f"æ‚¨å¥½ï¼æˆ‘æ˜¯åŒ»è„‰é€šæ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}
    ]

# æ˜¾ç¤ºèŠå¤©è®°å½•
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# è·å–æ¨¡å‹é…ç½®
model_info = load_info("models")[selected_model]
print(f"æ¨¡å‹é…ç½®: {model_info}")
model_name = model_info["model_name"]
base_url = model_info["base_url"]
api_key = model_info["api_key"]

if selected_model == "DeepSeek-Chat":
    Model = ChatDeepSeek
else:
    Model = ChatOpenAI

# å®šä¹‰æ¨¡å‹
llm = Model(
    model=model_name,
    base_url=base_url,
    api_key=api_key,
    temperature=temperature,
    max_tokens=max_tokens,
    streaming=True
)

# å®šä¹‰æç¤ºæ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt + """\nå·²çŸ¥å†…å®¹ï¼š{RAG}ï¼Œ
                                å¦‚æœæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·ä¼˜å…ˆä»â€œ{RAG}â€ä¸­æå–å†…å®¹ï¼Œæ•´åˆåŠ å·¥åå›ç­”ï¼Œå¹¶æ ‡æ˜çŸ¥è¯†æ¥è‡ªæ£€ç´¢åº“ã€‚
                                å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œå¹¶æ ‡æ˜çŸ¥è¯†æ¥è‡ªå¤§æ¨¡å‹ã€‚
                             """),
    ("human", "{user_input}")
])

if rag_flag:

    chain = ({
                 "RAG": RunnableLambda(itemgetter("user_input")) | get_context_from_db,
                 "user_input": itemgetter("user_input")
             }
             | prompt_template
             | llm
             | StrOutputParser()
             )
else:
    chain = ({
                 "RAG": itemgetter("user_input"),
                 "user_input": itemgetter("user_input")
              }
             | prompt_template
             | llm
             | StrOutputParser()
             )
# print(f'chain{chain}')

# å¤„ç†ç”¨æˆ·è¾“å…¥
if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "user", "content": question})
    st.chat_message("user").write(question)

    # è·å–æœºå™¨äººå›å¤ (æ˜¾ç¤ºåŠ è½½çŠ¶æ€)
    with st.chat_message("assistant"):
        try:
            start_time = time()
            print(f"å¼€å§‹å¤„ç†æ¶ˆæ¯: {question}")
            with st.spinner("æ€è€ƒä¸­..."):
                # åˆ›å»ºç©ºç™½å ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
                response_placeholder = st.empty()
                response_content = ""
                # ä½¿ç”¨LangChainå¤„ç†æ¶ˆæ¯å¹¶å®ç°æµå¼è¾“å‡º
                for chunk in chain.stream({"user_input": question}):
                    response_content += chunk
                    response_placeholder.markdown(response_content + "â–Œ")  # æ·»åŠ å…‰æ ‡æ•ˆæœ

                # å®Œæˆåæ˜¾ç¤ºæœ€ç»ˆå†…å®¹
                import re

                think_content_match = re.search(r'<think>(.*?)</think>', response_content, re.DOTALL)
                if think_content_match:
                    think_text = think_content_match.group(1).strip()
                    # ç§»é™¤åŸå§‹response_contentä¸­çš„thinkæ ‡ç­¾å†…å®¹
                    response_content_without_think = re.sub(r'<think>.*?</think>', '', response_content,
                                                            flags=re.DOTALL)
                    st.markdown("\n")  # åœ¨</think>æ ‡ç­¾åè¿›è¡Œæ¢è¡Œ
                    with st.expander("ç‚¹å‡»æŸ¥çœ‹æ€è€ƒå†…å®¹"):  # è®¾ç½®ç‚¹å‡»å¯ä»¥è¿›è¡Œæ”¶èµ·æ¥
                        st.markdown(think_text)
                    response_placeholder.markdown(response_content_without_think)
                else:
                    response_placeholder.markdown(response_content)
                end_time = time()
                duration = (end_time - start_time).__trunc__()
                print(f"æ¶ˆæ¯å¤„ç†å®Œæˆ: {question}, è€—æ—¶: {duration}ç§’")
                st.markdown("---")
                st.markdown(f"å¤„ç†è€—æ—¶{duration}ç§’")
        except Exception as e:
            print(f"Error: {e}")
            response_placeholder.markdown(f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚{e}")

    # å°†æœºå™¨äººå›å¤æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "assistant", "content": response_content})

# æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’®
if len(st.session_state.messages) > 1:
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = [
            {"role": "assistant", "content": f"æ‚¨å¥½ï¼æˆ‘æ˜¯åŒ»è„‰é€šæ™ºèƒ½åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}
        ]
        st.rerun()
